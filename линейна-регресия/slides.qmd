---
title: "Линейна регресия"
author: "Виктор Аврамов"
format:
  revealjs:
    incremental: true   
editor: visual
---

## Въведение {.scrollable}

- ### Дефиниция и цел

- Линейната регресия се използва за моделиране на връзката между зависима променлива и една или повече независими променливи. Основната цел на линейната регресия е да предвиди или оцени стойността на зависимата променлива при промени на стойностите на независимите променливи.

- ### Примери

- **Прогнозиране на продажбите:** Оценка на бъдещи продажби въз основа на разходите за реклама.
- **Управление на риска:** Оценка на риска от неизпълнение на кредита въз основа на финансовата история на кандидата.
- **Оценка на разходите:** Прогнозиране на разходите за проект въз основа на неговия обхват и продължителност.

- ### Математическа формулировка

- При обикновената линейна регресия връзката между зависимата променлива $Y$ и независимата променлива $X$ може да бъде изразена като:

- $$ Y = \beta_0 + \beta_1 X + \epsilon $$

- където:
- $Y$ е зависимата променлива.
- $X$ е независимата променлива.
- $\beta_0$ е пресечната точка с ординатата.
- $\beta_1$ е наклонът.
- $\epsilon$ е стандартната грешка на оценката.

- ### Пример с данни

- Нека разгледаме данни, с които искаме да прогнозираме годишните продажби (в хиляди евро) въз основа на рекламния бюджет (в хиляди евро). Една малка извадка от данните може да бъде:

| Рекламен бюджет ($X$) | Годишни продажби ($Y$) |
|--------------------------|--------------------|
| 10 | 25 |
| 15 | 35 |
| 20 | 45 |
| 25 | 55 |
| 30 | 65 |

- Може да визуализираме тези данни, и да покажем линейната връзка и регресионната линия.

## Примери

```{r}
# Load necessary library
library(ggplot2)

# Create the dataset
data <- data.frame(
  Advertising_Budget = c(10, 15, 20, 25, 30),
  Annual_Sales = c(25, 35, 45, 55, 65)
)

# Fit the linear model
model <- lm(Annual_Sales ~ Advertising_Budget, data = data)

# Get the coefficients of the model
intercept <- coef(model)[1]
slope <- coef(model)[2]

# Create the equation label
eq <- paste0("y = ", round(intercept, 2), " + ", round(slope, 2), "x")

# Create the scatter plot with regression line
p <- ggplot(data, aes(x = Advertising_Budget, y = Annual_Sales)) +
  geom_point(size = 3) +  # Scatter plot points
  geom_smooth(method = "lm", se = FALSE, color = "blue") +  # Regression line
  annotate("text", x = 25, y = 30, label = eq, parse = FALSE, color = "blue") +  # Equation
  labs(title = "",
       x = "Рекламен бюджет ($X$)",
       y = "Годишни продажби ($Y$)") +
  theme_minimal()

# Print the plot
print(p)


```


## Валидност на линейната регресия {.scrollable}

- За да може линейната регресия да осигури надеждни резултати, трябва да бъдат изпълнени определени допускания относно данните. Тези допускания гарантират, че моделът е валиден и оценките са неизместени.

- ### Линейност

- Връзката между независимата променлива ($X$) и зависимата променлива ($Y$) трябва да бъде линейна. Това означава, че промяна в $X$ води до пропорционална промяна в $Y$.

- #### Проверка на линейността

- Диаграма на разсейване на $X$ спрямо $Y$ може да покаже дали допускането за линейност е изпълнено. Ако графиката показва линейна тенденция, това вероятно е така.

- ### Независимост

- Наблюденията трябва да са независими едно от друго. Това означава, че стойността на $Y$ за едно наблюдение не се влияе от стойността на $Y$ за друго наблюдение.

- #### Проверка на независимостта

- Независимостта често може да бъде осигурена чрез подходящи методи за събиране на данни. Ако наблюденията се събират на случаен принцип, те вероятно ще бъдат независими.

- ### Хомоскедастичност

- Дисперсията на остатъците (разликите между наблюдаваните и прогнозираните стойности на $Y$) трябва да бъде постоянна за всички нива на $X$. Това означава, че разпространението на остатъците трябва да бъде приблизително еднакво за всички стойности на $X$.

- #### Проверка на хомоскедастичността

- Графика на остатъците от регресията стойности може да се използва за проверка. Ако остатъците са случайно разпределени около нулата без ясна тенденция, това допускане вероятно е изпълнено.

- ### Нормалност на остатъците

- Остатъците трябва да са приблизително нормално разпределени. Това предположение е важно за правене на статистически изводи относно коефициентите на регресията.

- #### Проверка на нормалността

- Хистограма или Q-Q диаграма на остатъците може да се използва за проверка за нормалност. Ако остатъците образуват камбанообразна крива или падат по права линия в Q-Q диаграма, това допускане вероятно е изпълнено.

## Пример за проверка на предположения с данни

Нека използваме същия набор от данни, за да проверим тези предположения:

```{r}
# Load necessary library
library(ggplot2)

# Create the dataset
data <- data.frame(
  Advertising_Budget = c(10, 15, 20, 25, 30),
  Annual_Sales = c(25, 35, 45, 55, 65)
)

# Fit the linear model
model <- lm(Annual_Sales ~ Advertising_Budget, data = data)

# Create residual plots
par(mfrow = c(2, 2))  # Arrange plots in a 2x2 grid

# 1. Scatter plot with regression line (for linearity)
plot(data$Advertising_Budget, data$Annual_Sales,
     main = "Scatter Plot with Regression Line",
     xlab = "Рекламен бюджет ($X$)",
     ylab = "Годишни продажби ($Y$)")
abline(model, col = "blue")

# 2. Residuals vs Fitted (for homoscedasticity)
plot(model$fitted.values, model$residuals,
     main = "Residuals vs Fitted",
     xlab = "Fitted values",
     ylab = "Residuals")
abline(h = 0, col = "red")

# 3. Q-Q plot of residuals (for normality)
qqnorm(model$residuals)
qqline(model$residuals, col = "red")

# 4. Histogram of residuals (for normality)
hist(model$residuals, breaks = 10, main = "Histogram of Residuals",
     xlab = "Residuals")

# Reset plotting layout
par(mfrow = c(1, 1))

```


## Интерпретиране {.scrollable}

- Регресионното уравнение е:

- $$Y = \beta_0 + \beta_1 X + \epsilon$$

- където:
- $Y$ е зависимата променлива.
- $X$ е независимата променлива.
- $\beta_0$ е пресечната точка с ординатата.
- $\beta_1$ е наклонът.
- $\epsilon$ е стандартната грешка на оценката.

## Пример: Реклама и продажби {.scrollable}

Нека разгледаме набор от данни, където имаме разходи за реклама ($X$) и приходи от продажби ($Y$). 

| Разходи за реклама ($X$) | Приходи от продажби ($Y$).  |
|-----------------------|-------------------|
| 23                    | 25                |
| 45                    | 49                |
| 34                    | 37                |
| 76                    | 80                |
| 54                    | 55                |
| 23                    | 25                |
| 56                    | 59                |
| 89                    | 94                |
| 43                    | 45                |
| 77                    | 82                |

## Пример: Реклама и продажби, R код


```{r}
# Sample data
advertising <- c(23, 45, 34, 76, 54, 23, 56, 89, 43, 77)
sales <- c(25, 49, 37, 80, 55, 25, 59, 94, 45, 82)

# Create a data frame
data <- data.frame(advertising, sales)

# Perform linear regression
model <- lm(sales ~ advertising, data = data)

# Summary of the model
summary(model)

# Get the regression equation
intercept <- coef(model)[1]
slope <- coef(model)[2]
regression_eq <- paste0("Y == ", round(intercept, 2), " + ", round(slope, 2), " * X")
```


## Пример: Реклама и продажби


```{r}
library(ggplot2)

# Scatter plot with regression line and equation text
ggplot(data, aes(x = advertising, y = sales)) +
  geom_point() +
  geom_smooth(method = "lm", col = "blue") +
  annotate("text", x = 50, y = 30, label = regression_eq, color = "red", size = 5, parse = TRUE) +
  labs(title = "",
       x = "Разходи за реклама",
       y = "Приходи от продажби")
```


## Интерпретиране

- ($\beta_0$): очакваната стойност на $Y$ (продажби), когато $X$ (разходи за реклама) е нула.

- ($\beta_1$): очакваната промяна в $Y$ за промяна с една единица в $X$. Положителният наклон предполага положителна връзка, докато отрицателният наклон предполага отрицателна връзка.
- ($\epsilon$): Това представлява вариацията в $Y$, която не може да бъде обяснена с $X$.

## Множествена линейна регресия {.scrollable}

- ### Концепция

- Множествената линейна регресия е разширение на простата линейна регресия, което ни позволява да изследваме връзката между зависима променлива ($Y$) и множество независими променливи ($X_1, X_2, \ldots, X_n$).

- ### Формулировка

- Регресионното уравнение за множествена линейна регресия е:

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_n X_n + \epsilon
$$

- където:
- $Y$ е зависимата променлива.
- $X_1, X_2, \ldots, X_n$ са независимите променливи.
- $\beta_0$ е пресечната точка с ординатата
- $\beta_1, \beta_2, \ldots, \beta_n$ са коефициентите (наклони) за всяка независима променлива.
- $\epsilon$ е стандартната грешка на оценката.

- ### Пример: Прогнозиране на продажби с разходи за реклама и размер на пазара
- Нека разгледаме набор от данни, в който са дадени разходи за реклама ($X_1$), размер на пазара ($X_2$) и приходи от продажби ($Y$).


| Разходи за реклама ($X_1$) | Пазарен размер ($X_2$) | Приходи от продажби ($Y$) |
|----------------------------|---------------------- |---------------------|
| 23 | 100 | 25 |
| 45 | 200 | 49 |
| 34 | 150 | 37 |
| 76 | 300 | 80 |
| 54 | 250 | 55 |
| 23 | 100 | 25 |
| 56 | 270 | 59 |
| 89 | 350 | 94 |
| 43 | 210 | 45 |
| 77 | 310 | 82 |


```{r}

# Sample data
advertising <- c(23, 45, 34, 76, 54, 23, 56, 89, 43, 77)
market_size <- c(100, 200, 150, 300, 250, 100, 270, 350, 210, 310)
sales <- c(25, 49, 37, 80, 55, 25, 59, 94, 45, 82)

# Create a data frame
data <- data.frame(advertising, market_size, sales)

# Perform multiple linear regression
model <- lm(sales ~ advertising + market_size, data = data)

# Summary of the model
summary(model)

# Get the regression equation
intercept <- coef(model)[1]
slope_advertising <- coef(model)[2]
slope_market_size <- coef(model)[3]
regression_eq <- paste0("Y = ", round(intercept, 2), " + ", round(slope_advertising, 2), " * X_1 + ", round(slope_market_size, 2), " * X_2")

```

## Интерпретиране {.scrollable}

- ($\beta_0$):  очакваната стойност на $Y$ (продажби), когато всички $X$ променливи (разходи за реклама и размер на пазара) са нула.
- ($\beta_1, \beta_2$):  очакваната промяна в $Y$ за промяна с една единица съответно в $X_1$ (разходи за реклама) и $X_2$ (размер на пазара).
- ($\epsilon$): Това представлява вариацията в $Y$, която не може да бъде обяснена с променливите $X$.

## Интерпретиране - продължение {.scrollable}

- **Estimate:** Прогнозната стойност на коефициента (напр. $\beta_0$, $\beta_1$).
- **Std. Error:** Стандартната грешка на оценката, показваща променливостта на оценката на коефициента.
- **t value:** t-статистиката за тестване на нулевата хипотеза, че коефициентът е нула.
- **Pr(>|t|):** p-стойността за t-теста, показваща значимостта на коефициента.

- За всеки предиктор:

- Ниска p-стойност (обикновено < 0,05) показва, че предикторът е статистически значим.
- Знакът на оценката (положителен или отрицателен) показва посоката на връзката със зависимата променлива.

- **Коефициент на детерминация ($R^2$):** представлява съотношението на дисперсията в зависимата променлива ($Y$), която може да се обясни с независимите променливи ($X$). Варира от 0 до 1, където:
 - $R^2 = 0$ означава, че независимите променливи не обясняват никоя от променливостта в зависимата променлива.
 - $R^2 = 1$ означава, че независимите променливи обясняват цялата променливост на зависимата променлива.
 - Като цяло по-високата стойност на $R^2$ показва по-добро съответствие на модела с данните.

- **Изравнен коефициент на детерминация ($\bar{R}^2$):** Това е модифицирана версия на $R^2$, която отчита броя на предикторите в модела.
 - $\bar{R}^2$ може да бъде по-ниско от $R^2$, ако добавени предиктори не подобряват модела.
 - $\bar{R}^2$ винаги е по-малко или равно на $R^2$.
 
## Моделиране на практика

- Линейната регресия е прост и мощен инструмент за анализ на данни, но е важно да сме наясно с допусканията за валидност и с най-добрите практики, за да сме сигурни, че нашият анализ е съгласуван с данните.

- ### Често срещани грешки

 - 1. Пренебрегване на допусканията за линейност, независимост, хомоскедастичност, нормалност и липса на мултиколинеарност.

- Стандартизирането (или нормализирането) на данните може да помогне за подобряване на интерпретируемостта на коефициентите и числената стабилност на модела.

- ### 2. Overfitting

- ### 3. Мултиколинеарност
Мултиколинеарност имаме, когато независимите променливи са силно корелирани една с друга, което затруднява оценката на техните индивидуални ефекти.

- ### 4. Изключителни наблюдения и грешки в данните

- ### 5. Неправилно тълкуване на коефициенти

- ### 6. Нелинейна връзка

- ### 7. Корелация и каузалност